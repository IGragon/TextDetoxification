{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf TextDetoxification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHfJlpTPTIDE",
        "outputId": "7ae68bf3-6298-4886-d4a0-22a69605180d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu9pGROkxJJs",
        "outputId": "ee4d9d5b-2f1a-4605-a11a-a2cb15903d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TextDetoxification'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 102 (delta 27), reused 78 (delta 20), pack-reused 17\u001b[K\n",
            "Receiving objects: 100% (102/102), 42.84 MiB | 44.00 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n",
            "Filtering content: 100% (4/4), 335.58 MiB | 53.02 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IGragon/TextDetoxification.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r TextDetoxification/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu9Ka1fgSSo9",
        "outputId": "45a3283c-4a31-43ce-dd58-62901076c8c8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas~=2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: tqdm~=4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: datasets~=2.14.6 in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 3)) (2.14.6)\n",
            "Requirement already satisfied: transformers~=4.34.1 in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 4)) (4.34.1)\n",
            "Requirement already satisfied: numpy~=1.26.1 in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 5)) (1.26.1)\n",
            "Requirement already satisfied: evaluate~=0.4.1 in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: nltk~=3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 9)) (0.1.99)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (from -r TextDetoxification/requirements.txt (line 10)) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.1.2->-r TextDetoxification/requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.1.2->-r TextDetoxification/requirements.txt (line 1)) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.1.2->-r TextDetoxification/requirements.txt (line 1)) (2023.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers~=4.34.1->-r TextDetoxification/requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.34.1->-r TextDetoxification/requirements.txt (line 4)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.34.1->-r TextDetoxification/requirements.txt (line 4)) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.34.1->-r TextDetoxification/requirements.txt (line 4)) (0.4.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate~=0.4.1->-r TextDetoxification/requirements.txt (line 6)) (0.18.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk~=3.8.1->-r TextDetoxification/requirements.txt (line 7)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk~=3.8.1->-r TextDetoxification/requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (0.16.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (1.11.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r TextDetoxification/requirements.txt (line 10)) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r TextDetoxification/requirements.txt (line 10)) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets~=2.14.6->-r TextDetoxification/requirements.txt (line 3)) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers->-r TextDetoxification/requirements.txt (line 8)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "tdoAtXCPSh33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TextDetoxification/src/models/\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "CZVJCWY4SXTa",
        "outputId": "f85f7910-c322-45c2-acf3-e0c95d05f672"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TextDetoxification/src/models\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/TextDetoxification/src/models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train_model.py --num_train_epochs 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eghuLSRESnxm",
        "outputId": "611ce1f8-dfd0-4058-dcd1-58c1261c7ef2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 12:39:42.909475: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-04 12:39:42.909527: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-04 12:39:42.909564: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-04 12:39:44.473220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
            "  if _pandas_api.is_sparse(col):\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Map: 100% 1600/1600 [00:00<00:00, 4555.12 examples/s]\n",
            "Map: 100% 400/400 [00:00<00:00, 5456.95 examples/s]\n",
            "  0% 0/300 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            " 33% 100/300 [00:28<00:55,  3.58it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:11,  4.08it/s]\u001b[A\n",
            "  6% 3/50 [00:01<00:16,  2.81it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.35it/s]\u001b[A\n",
            " 10% 5/50 [00:02<00:23,  1.94it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:25,  1.76it/s]\u001b[A\n",
            " 14% 7/50 [00:03<00:26,  1.63it/s]\u001b[A\n",
            " 16% 8/50 [00:04<00:28,  1.48it/s]\u001b[A\n",
            " 18% 9/50 [00:05<00:29,  1.40it/s]\u001b[A\n",
            " 20% 10/50 [00:06<00:29,  1.35it/s]\u001b[A\n",
            " 22% 11/50 [00:06<00:29,  1.31it/s]\u001b[A\n",
            " 24% 12/50 [00:07<00:28,  1.35it/s]\u001b[A\n",
            " 26% 13/50 [00:08<00:24,  1.49it/s]\u001b[A\n",
            " 28% 14/50 [00:08<00:22,  1.61it/s]\u001b[A\n",
            " 30% 15/50 [00:09<00:20,  1.72it/s]\u001b[A\n",
            " 32% 16/50 [00:09<00:18,  1.80it/s]\u001b[A\n",
            " 34% 17/50 [00:10<00:17,  1.87it/s]\u001b[A\n",
            " 36% 18/50 [00:10<00:16,  1.91it/s]\u001b[A\n",
            " 38% 19/50 [00:11<00:16,  1.92it/s]\u001b[A\n",
            " 40% 20/50 [00:11<00:15,  1.93it/s]\u001b[A\n",
            " 42% 21/50 [00:12<00:14,  1.95it/s]\u001b[A\n",
            " 44% 22/50 [00:12<00:14,  1.96it/s]\u001b[A\n",
            " 46% 23/50 [00:13<00:13,  1.98it/s]\u001b[A\n",
            " 48% 24/50 [00:13<00:13,  1.99it/s]\u001b[A\n",
            " 50% 25/50 [00:13<00:11,  2.09it/s]\u001b[A\n",
            " 52% 26/50 [00:14<00:11,  2.07it/s]\u001b[A\n",
            " 54% 27/50 [00:14<00:11,  2.04it/s]\u001b[A\n",
            " 56% 28/50 [00:15<00:10,  2.04it/s]\u001b[A\n",
            " 58% 29/50 [00:15<00:10,  2.04it/s]\u001b[A\n",
            " 60% 30/50 [00:16<00:09,  2.06it/s]\u001b[A\n",
            " 62% 31/50 [00:16<00:09,  2.04it/s]\u001b[A\n",
            " 64% 32/50 [00:17<00:08,  2.01it/s]\u001b[A\n",
            " 66% 33/50 [00:18<00:09,  1.83it/s]\u001b[A\n",
            " 68% 34/50 [00:18<00:09,  1.70it/s]\u001b[A\n",
            " 70% 35/50 [00:19<00:09,  1.61it/s]\u001b[A\n",
            " 72% 36/50 [00:20<00:09,  1.47it/s]\u001b[A\n",
            " 74% 37/50 [00:21<00:09,  1.39it/s]\u001b[A\n",
            " 76% 38/50 [00:21<00:09,  1.33it/s]\u001b[A\n",
            " 78% 39/50 [00:22<00:08,  1.29it/s]\u001b[A\n",
            " 80% 40/50 [00:23<00:07,  1.33it/s]\u001b[A\n",
            " 82% 41/50 [00:23<00:06,  1.47it/s]\u001b[A\n",
            " 84% 42/50 [00:24<00:04,  1.61it/s]\u001b[A\n",
            " 86% 43/50 [00:24<00:04,  1.71it/s]\u001b[A\n",
            " 88% 44/50 [00:25<00:03,  1.79it/s]\u001b[A\n",
            " 90% 45/50 [00:25<00:02,  1.86it/s]\u001b[A\n",
            " 92% 46/50 [00:26<00:01,  2.00it/s]\u001b[A\n",
            " 94% 47/50 [00:26<00:01,  2.00it/s]\u001b[A\n",
            " 96% 48/50 [00:27<00:00,  2.01it/s]\u001b[A\n",
            " 98% 49/50 [00:27<00:00,  2.01it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.951487421989441, 'eval_rouge1': 0.5486081749544376, 'eval_rouge2': 0.3077241915039115, 'eval_rougeL': 0.5260073975628117, 'eval_rougeLsum': 0.526118314999298, 'eval_runtime': 31.5559, 'eval_samples_per_second': 12.676, 'eval_steps_per_second': 1.584, 'epoch': 1.0}\n",
            " 33% 100/300 [00:59<00:55,  3.58it/s]\n",
            "100% 50/50 [00:28<00:00,  2.04it/s]\u001b[A\n",
            " 67% 200/300 [01:23<00:30,  3.26it/s]\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:20,  2.30it/s]\u001b[A\n",
            "  6% 3/50 [00:01<00:27,  1.74it/s]\u001b[A\n",
            "  8% 4/50 [00:02<00:31,  1.46it/s]\u001b[A\n",
            " 10% 5/50 [00:03<00:33,  1.33it/s]\u001b[A\n",
            " 12% 6/50 [00:04<00:34,  1.26it/s]\u001b[A\n",
            " 14% 7/50 [00:05<00:34,  1.23it/s]\u001b[A\n",
            " 16% 8/50 [00:05<00:34,  1.23it/s]\u001b[A\n",
            " 18% 9/50 [00:06<00:33,  1.24it/s]\u001b[A\n",
            " 20% 10/50 [00:07<00:29,  1.35it/s]\u001b[A\n",
            " 22% 11/50 [00:07<00:25,  1.50it/s]\u001b[A\n",
            " 24% 12/50 [00:08<00:23,  1.63it/s]\u001b[A\n",
            " 26% 13/50 [00:08<00:21,  1.73it/s]\u001b[A\n",
            " 28% 14/50 [00:09<00:20,  1.80it/s]\u001b[A\n",
            " 30% 15/50 [00:09<00:18,  1.86it/s]\u001b[A\n",
            " 32% 16/50 [00:10<00:17,  1.91it/s]\u001b[A\n",
            " 34% 17/50 [00:10<00:16,  1.94it/s]\u001b[A\n",
            " 36% 18/50 [00:11<00:16,  1.97it/s]\u001b[A\n",
            " 38% 19/50 [00:11<00:15,  1.99it/s]\u001b[A\n",
            " 40% 20/50 [00:12<00:15,  1.99it/s]\u001b[A\n",
            " 42% 21/50 [00:12<00:14,  2.02it/s]\u001b[A\n",
            " 44% 22/50 [00:13<00:14,  2.00it/s]\u001b[A\n",
            " 46% 23/50 [00:13<00:13,  2.01it/s]\u001b[A\n",
            " 48% 24/50 [00:14<00:12,  2.01it/s]\u001b[A\n",
            " 50% 25/50 [00:14<00:11,  2.12it/s]\u001b[A\n",
            " 52% 26/50 [00:15<00:11,  2.05it/s]\u001b[A\n",
            " 54% 27/50 [00:15<00:12,  1.84it/s]\u001b[A\n",
            " 56% 28/50 [00:16<00:12,  1.71it/s]\u001b[A\n",
            " 58% 29/50 [00:17<00:12,  1.62it/s]\u001b[A\n",
            " 60% 30/50 [00:18<00:13,  1.50it/s]\u001b[A\n",
            " 62% 31/50 [00:18<00:13,  1.41it/s]\u001b[A\n",
            " 64% 32/50 [00:19<00:13,  1.34it/s]\u001b[A\n",
            " 66% 33/50 [00:20<00:13,  1.30it/s]\u001b[A\n",
            " 68% 34/50 [00:21<00:11,  1.33it/s]\u001b[A\n",
            " 70% 35/50 [00:21<00:10,  1.48it/s]\u001b[A\n",
            " 72% 36/50 [00:22<00:08,  1.63it/s]\u001b[A\n",
            " 74% 37/50 [00:22<00:07,  1.73it/s]\u001b[A\n",
            " 76% 38/50 [00:23<00:06,  1.81it/s]\u001b[A\n",
            " 78% 39/50 [00:23<00:05,  1.88it/s]\u001b[A\n",
            " 80% 40/50 [00:24<00:05,  1.92it/s]\u001b[A\n",
            " 82% 41/50 [00:24<00:04,  1.96it/s]\u001b[A\n",
            " 84% 42/50 [00:25<00:04,  1.98it/s]\u001b[A\n",
            " 86% 43/50 [00:25<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:26<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:26<00:02,  1.98it/s]\u001b[A\n",
            " 92% 46/50 [00:27<00:01,  2.10it/s]\u001b[A\n",
            " 94% 47/50 [00:27<00:01,  2.08it/s]\u001b[A\n",
            " 96% 48/50 [00:28<00:00,  2.05it/s]\u001b[A\n",
            " 98% 49/50 [00:28<00:00,  2.03it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.8958616256713867, 'eval_rouge1': 0.5504604633165262, 'eval_rouge2': 0.3099207419214717, 'eval_rougeL': 0.5282248971579147, 'eval_rougeLsum': 0.5283740457778685, 'eval_runtime': 30.3911, 'eval_samples_per_second': 13.162, 'eval_steps_per_second': 1.645, 'epoch': 2.0}\n",
            " 67% 200/300 [01:54<00:30,  3.26it/s]\n",
            "100% 50/50 [00:29<00:00,  2.02it/s]\u001b[A\n",
            "100% 300/300 [02:18<00:00,  4.48it/s]\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:11,  4.18it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:16,  2.87it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:18,  2.53it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.34it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:19,  2.25it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:19,  2.18it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:20,  2.05it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:22,  1.85it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:23,  1.71it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:24,  1.61it/s]\u001b[A\n",
            " 24% 12/50 [00:06<00:25,  1.47it/s]\u001b[A\n",
            " 26% 13/50 [00:07<00:26,  1.38it/s]\u001b[A\n",
            " 28% 14/50 [00:07<00:27,  1.32it/s]\u001b[A\n",
            " 30% 15/50 [00:08<00:27,  1.28it/s]\u001b[A\n",
            " 32% 16/50 [00:09<00:26,  1.28it/s]\u001b[A\n",
            " 34% 17/50 [00:10<00:26,  1.26it/s]\u001b[A\n",
            " 36% 18/50 [00:11<00:24,  1.29it/s]\u001b[A\n",
            " 38% 19/50 [00:11<00:24,  1.29it/s]\u001b[A\n",
            " 40% 20/50 [00:12<00:23,  1.26it/s]\u001b[A\n",
            " 42% 21/50 [00:13<00:23,  1.25it/s]\u001b[A\n",
            " 44% 22/50 [00:14<00:22,  1.25it/s]\u001b[A\n",
            " 46% 23/50 [00:15<00:21,  1.28it/s]\u001b[A\n",
            " 48% 24/50 [00:15<00:18,  1.44it/s]\u001b[A\n",
            " 50% 25/50 [00:16<00:15,  1.64it/s]\u001b[A\n",
            " 52% 26/50 [00:16<00:13,  1.75it/s]\u001b[A\n",
            " 54% 27/50 [00:16<00:12,  1.84it/s]\u001b[A\n",
            " 56% 28/50 [00:17<00:11,  1.89it/s]\u001b[A\n",
            " 58% 29/50 [00:17<00:10,  1.94it/s]\u001b[A\n",
            " 60% 30/50 [00:18<00:10,  1.99it/s]\u001b[A\n",
            " 62% 31/50 [00:18<00:09,  2.02it/s]\u001b[A\n",
            " 64% 32/50 [00:19<00:08,  2.02it/s]\u001b[A\n",
            " 66% 33/50 [00:19<00:08,  2.02it/s]\u001b[A\n",
            " 68% 34/50 [00:20<00:07,  2.00it/s]\u001b[A\n",
            " 70% 35/50 [00:20<00:07,  2.02it/s]\u001b[A\n",
            " 72% 36/50 [00:21<00:06,  2.01it/s]\u001b[A\n",
            " 74% 37/50 [00:21<00:06,  2.02it/s]\u001b[A\n",
            " 76% 38/50 [00:22<00:05,  2.03it/s]\u001b[A\n",
            " 78% 39/50 [00:22<00:05,  2.01it/s]\u001b[A\n",
            " 80% 40/50 [00:23<00:04,  2.01it/s]\u001b[A\n",
            " 82% 41/50 [00:23<00:04,  2.03it/s]\u001b[A\n",
            " 84% 42/50 [00:24<00:03,  2.02it/s]\u001b[A\n",
            " 86% 43/50 [00:24<00:03,  2.04it/s]\u001b[A\n",
            " 88% 44/50 [00:25<00:03,  1.85it/s]\u001b[A\n",
            " 90% 45/50 [00:26<00:02,  1.73it/s]\u001b[A\n",
            " 92% 46/50 [00:26<00:02,  1.76it/s]\u001b[A\n",
            " 94% 47/50 [00:27<00:01,  1.61it/s]\u001b[A\n",
            " 96% 48/50 [00:28<00:01,  1.47it/s]\u001b[A\n",
            " 98% 49/50 [00:29<00:00,  1.37it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.8862723112106323, 'eval_rouge1': 0.5546435511781229, 'eval_rouge2': 0.3117926971350774, 'eval_rougeL': 0.5323964962340615, 'eval_rougeLsum': 0.5327085969585399, 'eval_runtime': 31.4113, 'eval_samples_per_second': 12.734, 'eval_steps_per_second': 1.592, 'epoch': 3.0}\n",
            "100% 300/300 [02:50<00:00,  4.48it/s]\n",
            "100% 50/50 [00:30<00:00,  1.33it/s]\u001b[A\n",
            "{'train_runtime': 170.2301, 'train_samples_per_second': 28.197, 'train_steps_per_second': 1.762, 'train_loss': 1.998310546875, 'epoch': 3.0}\n",
            "100% 300/300 [02:50<00:00,  1.76it/s]\n",
            "TrainOutput(global_step=300, training_loss=1.998310546875, metrics={'train_runtime': 170.2301, 'train_samples_per_second': 28.197, 'train_steps_per_second': 1.762, 'train_loss': 1.998310546875, 'epoch': 3.0})\n",
            "100% 50/50 [00:27<00:00,  1.80it/s]\n",
            "{'eval_loss': 1.8862723112106323, 'eval_rouge1': 0.5546435511781229, 'eval_rouge2': 0.3117926971350774, 'eval_rougeL': 0.5323964962340615, 'eval_rougeLsum': 0.5327085969585399, 'eval_runtime': 28.2611, 'eval_samples_per_second': 14.154, 'eval_steps_per_second': 1.769, 'epoch': 3.0}\n",
            "pytorch_model.bin: 100% 892M/892M [00:22<00:00, 39.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8tc1li5WF6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "YHddctLGbChM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/TextDetoxification/src/models/\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VVxqVtPmbEi7",
        "outputId": "1c8c50e0-2885-485d-9855-a6efa605bced"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TextDetoxification/src/models\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/TextDetoxification/src/models'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 evaluate_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsK6LHGObKDS",
        "outputId": "27bceb11-8976-48bb-d047-34f6e0ec19b8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-04 13:09:34.556811: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-04 13:09:34.556887: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-04 13:09:34.556929: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-04 13:09:36.439525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Please, I'll sleep in the bloody bathtub.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "[\"please, I'll sleep in the bloody bathtub.\", \"I'll sleep in the seaside bathtub.\", 'me, sleep in the bloody tub.', \"I'll sleep in the bloody bathtub.\", 'I sleep in the bloody bathtub.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7N1h3kRobZXR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}